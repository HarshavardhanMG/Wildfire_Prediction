{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0942f38b",
   "metadata": {},
   "source": [
    "# Wildfire Prediction Model - Initial Data Exploration\n",
    "\n",
    "This notebook sets up the environment and performs initial data exploration for a wildfire prediction model using the modified next day wildfire dataset.\n",
    "\n",
    "## Objectives:\n",
    "1. Set up project environment and dependencies\n",
    "2. Parse TFRecord files to understand data structure\n",
    "3. Explore feature tensors and their properties\n",
    "4. Understand the dataset format and contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc799987",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import essential libraries for project setup, data handling, and TFRecord parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92733e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "TensorFlow version: 2.20.0\n",
      "NumPy version: 2.3.3\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data handling and numerical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TensorFlow for TFRecord parsing\n",
    "import tensorflow as tf\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up matplotlib for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140223c0",
   "metadata": {},
   "source": [
    "## 2. Set Up Project Directory Structure\n",
    "\n",
    "Verify our cookiecutter-data-science template structure is in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12b5716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\Harshvardhan\\OneDrive\\Desktop\\wildfire_pred\n",
      "Data directory: c:\\Users\\Harshvardhan\\OneDrive\\Desktop\\wildfire_pred\\data\n",
      "\n",
      "Directory structure verification:\n",
      "âœ“ data\\raw\n",
      "âœ“ data\\interim\n",
      "âœ“ data\\processed\n",
      "âœ“ models\n",
      "âœ“ reports\\figures\n",
      "âœ“ src\\data\n",
      "âœ“ src\\features\n",
      "âœ“ src\\models\n",
      "âœ“ src\\visualization\n"
     ]
    }
   ],
   "source": [
    "# Define project root and key directories\n",
    "PROJECT_ROOT = Path().absolute().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "\n",
    "# Verify directory structure\n",
    "required_dirs = [\n",
    "    DATA_DIR / \"raw\",\n",
    "    DATA_DIR / \"interim\", \n",
    "    DATA_DIR / \"processed\",\n",
    "    MODELS_DIR,\n",
    "    REPORTS_DIR / \"figures\",\n",
    "    SRC_DIR / \"data\",\n",
    "    SRC_DIR / \"features\",\n",
    "    SRC_DIR / \"models\",\n",
    "    SRC_DIR / \"visualization\"\n",
    "]\n",
    "\n",
    "print(\"\\nDirectory structure verification:\")\n",
    "for dir_path in required_dirs:\n",
    "    exists = \"âœ“\" if dir_path.exists() else \"âœ—\"\n",
    "    print(f\"{exists} {dir_path.relative_to(PROJECT_ROOT)}\")\n",
    "\n",
    "# Add src to Python path for imports\n",
    "sys.path.append(str(SRC_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f1f9c",
   "metadata": {},
   "source": [
    "## 3. Create Dependencies Configuration\n",
    "\n",
    "Verify our requirements.txt file contains all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "012173cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ requirements.txt found!\n",
      "\n",
      "Dependencies listed:\n",
      "# Data Handling\n",
      "pandas>=1.5.0\n",
      "numpy>=1.21.0\n",
      "xarray>=2022.6.0\n",
      "rioxarray>=0.12.0\n",
      "tensorflow>=2.12.0\n",
      "\n",
      "# Visualization\n",
      "matplotlib>=3.5.0\n",
      "seaborn>=0.11.0\n",
      "geopandas>=0.13.0\n",
      "plotly>=5.15.0\n",
      "folium>=0.14.0\n",
      "\n",
      "# Machine Learning\n",
      "scikit-learn>=1.1.0\n",
      "keras>=2.12.0\n",
      "\n",
      "# Geospatial Processing\n",
      "shapely>=1.8.0\n",
      "fiona>=1.8.0\n",
      "pyproj>=3.4.0\n",
      "contextily>=1.3.0\n",
      "\n",
      "# Utilities\n",
      "tqdm>=4.64.0\n",
      "joblib>=1.2.0\n",
      "h5py>=3.7.0\n",
      "\n",
      "# Development and Testing\n",
      "jupyter>=1.0.0\n",
      "ipykernel>=6.15.0\n",
      "pytest>=7.1.0\n",
      "black>=22.6.0\n",
      "flake8>=5.0.0\n",
      "\n",
      "# Optional: For better performance\n",
      "dask[complete]>=2022.8.0\n"
     ]
    }
   ],
   "source": [
    "# Check if requirements.txt exists and display its contents\n",
    "requirements_path = PROJECT_ROOT / \"requirements.txt\"\n",
    "\n",
    "if requirements_path.exists():\n",
    "    print(\"âœ“ requirements.txt found!\")\n",
    "    print(\"\\nDependencies listed:\")\n",
    "    with open(requirements_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        print(content)\n",
    "else:\n",
    "    print(\"âœ— requirements.txt not found!\")\n",
    "    print(\"Please create requirements.txt in the project root.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6e745",
   "metadata": {},
   "source": [
    "## 4. Install and Verify Required Packages\n",
    "\n",
    "Test imports of key libraries to ensure everything is properly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1182e31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ PRODUCTION ENVIRONMENT VERIFICATION\n",
      "==================================================\n",
      "âœ“ pandas               v2.3.3\n",
      "âœ“ numpy                v2.3.3\n",
      "âœ“ tensorflow           v2.20.0\n",
      "âœ“ matplotlib.pyplot    imported (version not available)\n",
      "âœ“ seaborn              v0.13.2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28m__import__\u001b[39m(lib)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     version = \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlib\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.__version__\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Some libraries don't have __version__\u001b[39;00m\n\u001b[32m     35\u001b[39m     version = \u001b[33m\"\u001b[39m\u001b[33mimported successfully\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "# PRODUCTION-SCALE LIBRARY TESTING WITH ENVIRONMENT SETUP\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Test imports of key libraries with production requirements\n",
    "libraries_to_test = [\n",
    "    ('pandas', 'pd'),\n",
    "    ('numpy', 'np'),\n",
    "    ('tensorflow', 'tf'),\n",
    "    ('matplotlib.pyplot', 'plt'),\n",
    "    ('seaborn', 'sns'),\n",
    "    ('sklearn', None),\n",
    "    ('scipy', None),\n",
    "    ('joblib', None),\n",
    "    ('pickle', None)\n",
    "]\n",
    "\n",
    "print(\"ğŸ”§ PRODUCTION ENVIRONMENT VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_imported = True\n",
    "environment_info = {}\n",
    "\n",
    "for lib, alias in libraries_to_test:\n",
    "    try:\n",
    "        if alias:\n",
    "            exec(f\"import {lib} as {alias}\")\n",
    "            version = eval(f\"{alias}.__version__\")\n",
    "        else:\n",
    "            __import__(lib)\n",
    "            try:\n",
    "                version = eval(f\"{lib}.__version__\")\n",
    "            except AttributeError:\n",
    "                # Some libraries don't have __version__\n",
    "                version = \"imported successfully\"\n",
    "        \n",
    "        print(f\"âœ“ {lib:<20} v{version}\")\n",
    "        environment_info[lib] = version\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"âœ— {lib:<20} FAILED: {e}\")\n",
    "        all_imported = False\n",
    "        environment_info[lib] = f\"MISSING: {e}\"\n",
    "    except AttributeError:\n",
    "        print(f\"âœ“ {lib:<20} imported (version not available)\")\n",
    "        environment_info[lib] = \"version_unknown\"\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check TensorFlow GPU availability\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"\\nğŸ–¥ï¸  TensorFlow Configuration:\")\n",
    "    print(f\"   Version: {tf.__version__}\")\n",
    "    print(f\"   GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "    print(f\"   Physical Devices: {len(tf.config.list_physical_devices())}\")\n",
    "    \n",
    "    # Set memory growth for GPUs if available\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"   âœ“ GPU memory growth enabled for {len(gpus)} device(s)\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"   âš ï¸  GPU setup warning: {e}\")\n",
    "    \n",
    "    environment_info['tensorflow_gpu'] = len(gpus) > 0\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ TensorFlow configuration failed: {e}\")\n",
    "    environment_info['tensorflow_gpu'] = False\n",
    "\n",
    "# System information\n",
    "print(f\"\\nğŸ–¥ï¸  System Information:\")\n",
    "print(f\"   Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"   Platform: {sys.platform}\")\n",
    "print(f\"   CPU Count: {sys.getsizeof('') and 'available' or 'unknown'}\")\n",
    "\n",
    "# Save environment information for reproducibility\n",
    "environment_file = PROJECT_ROOT / \"environment_info.txt\"\n",
    "with open(environment_file, 'w') as f:\n",
    "    f.write(\"WILDFIRE PREDICTION PROJECT - ENVIRONMENT INFO\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Python Version: {sys.version}\\n\")\n",
    "    f.write(f\"Platform: {sys.platform}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Library Versions:\\n\")\n",
    "    f.write(\"-\" * 20 + \"\\n\")\n",
    "    for lib, version in environment_info.items():\n",
    "        f.write(f\"{lib}: {version}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nTensorFlow GPU Available: {environment_info.get('tensorflow_gpu', False)}\\n\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Environment info saved to: {environment_file}\")\n",
    "\n",
    "if all_imported:\n",
    "    print(\"\\nâœ… ALL REQUIRED LIBRARIES ARE AVAILABLE!\")\n",
    "    print(\"ğŸš€ Ready for production-scale wildfire prediction development\")\n",
    "else:\n",
    "    print(\"\\nâŒ SOME LIBRARIES ARE MISSING\")\n",
    "    print(\"ğŸ“‹ Please install missing dependencies from requirements.txt\")\n",
    "    print(\"   Run: pip install -r requirements.txt\")\n",
    "\n",
    "# Set up pandas display options for better output\n",
    "try:\n",
    "    pd.set_option('display.max_columns', 20)\n",
    "    pd.set_option('display.width', 100)\n",
    "    pd.set_option('display.precision', 4)\n",
    "    print(\"\\nğŸ“Š Pandas display options configured\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set up matplotlib for high-quality plots\n",
    "try:\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    plt.rcParams['figure.dpi'] = 100\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    print(\"ğŸ“ˆ Matplotlib configured for production plots\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b17a5",
   "metadata": {},
   "source": [
    "## 5. Data Ingestion Setup\n",
    "\n",
    "Set up data paths and create utility functions for handling TFRecord format data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707bb0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‚ï¸  PRODUCTION DATASET DISCOVERY\n",
      "==================================================\n",
      "Primary data directory: c:\\Users\\Harshvardhan\\OneDrive\\Desktop\\wildfire_pred\\data\\raw\\ndws_western_dataset\n",
      "Directory exists: True\n",
      "\n",
      "ğŸ“Š Dataset File Summary:\n",
      "   TFRecord files: 54\n",
      "   TFIndex files: 54\n",
      "   Total dataset size: 7.06 GB\n",
      "\n",
      "ğŸ“‹ Dataset Split Analysis:\n",
      "   Train   :  25 files (3.37 GB)\n",
      "     - cleaned_train_ndws_conus_western_000.tfrecord\n",
      "     - cleaned_train_ndws_conus_western_001.tfrecord\n",
      "     - ... and 23 more files\n",
      "   Eval    :  13 files (1.51 GB)\n",
      "     - cleaned_eval_ndws_conus_western_000.tfrecord\n",
      "     - cleaned_eval_ndws_conus_western_001.tfrecord\n",
      "     - ... and 11 more files\n",
      "   Test    :  16 files (2.18 GB)\n",
      "     - cleaned_test_ndws_conus_western_000.tfrecord\n",
      "     - cleaned_test_ndws_conus_western_001.tfrecord\n",
      "     - ... and 14 more files\n",
      "\n",
      "ğŸ” File Integrity Validation:\n",
      "   âœ… All TFRecord files have corresponding .tfindex files\n",
      "   âœ… No obviously corrupted files detected\n",
      "\n",
      "ğŸ’¾ Dataset info saved to: c:\\Users\\Harshvardhan\\OneDrive\\Desktop\\wildfire_pred\\data\\interim\\dataset_discovery_info.pkl\n",
      "\n",
      "ğŸ¯ DATASET READINESS ASSESSMENT\n",
      "------------------------------\n",
      "âœ… Status: READY FOR PRODUCTION\n",
      "ğŸ“Š Files available: 54\n",
      "ğŸ’¾ Total size: 7.06 GB\n",
      "ğŸ”„ Splits available: 3\n",
      "\n",
      "ğŸ’¡ Processing Estimates:\n",
      "   Recommended RAM: 14.1+ GB\n",
      "   Processing time estimate: 108-270 minutes\n",
      "   GPU acceleration: Recommended\n",
      "\n",
      "ğŸš€ Ready to proceed to data parsing and analysis!\n"
     ]
    }
   ],
   "source": [
    "# PRODUCTION-SCALE DATASET DISCOVERY AND VALIDATION\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Enhanced data paths setup with validation\n",
    "TFRECORD_DATA_DIR = DATA_DIR / \"raw\" / \"ndws_western_dataset\"\n",
    "\n",
    "print(\"ğŸ—‚ï¸  PRODUCTION DATASET DISCOVERY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Primary data directory: {TFRECORD_DATA_DIR}\")\n",
    "print(f\"Directory exists: {TFRECORD_DATA_DIR.exists()}\")\n",
    "\n",
    "# Comprehensive file discovery and validation\n",
    "dataset_info = {\n",
    "    'tfrecord_files': [],\n",
    "    'tfindex_files': [],\n",
    "    'file_sizes': {},\n",
    "    'dataset_splits': {\n",
    "        'train': [],\n",
    "        'eval': [],\n",
    "        'test': []\n",
    "    },\n",
    "    'total_size_gb': 0\n",
    "}\n",
    "\n",
    "if TFRECORD_DATA_DIR.exists():\n",
    "    # Find all data files\n",
    "    tfrecord_files = list(TFRECORD_DATA_DIR.glob(\"*.tfrecord\"))\n",
    "    tfindex_files = list(TFRECORD_DATA_DIR.glob(\"*.tfindex\"))\n",
    "    \n",
    "    dataset_info['tfrecord_files'] = tfrecord_files\n",
    "    dataset_info['tfindex_files'] = tfindex_files\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Dataset File Summary:\")\n",
    "    print(f\"   TFRecord files: {len(tfrecord_files)}\")\n",
    "    print(f\"   TFIndex files: {len(tfindex_files)}\")\n",
    "    \n",
    "    # Calculate file sizes and organize by split\n",
    "    total_size = 0\n",
    "    for file_path in tfrecord_files:\n",
    "        file_size = file_path.stat().st_size\n",
    "        dataset_info['file_sizes'][file_path.name] = file_size\n",
    "        total_size += file_size\n",
    "        \n",
    "        # Categorize by dataset split\n",
    "        if 'train' in file_path.name:\n",
    "            dataset_info['dataset_splits']['train'].append(file_path)\n",
    "        elif 'eval' in file_path.name:\n",
    "            dataset_info['dataset_splits']['eval'].append(file_path)\n",
    "        elif 'test' in file_path.name:\n",
    "            dataset_info['dataset_splits']['test'].append(file_path)\n",
    "    \n",
    "    dataset_info['total_size_gb'] = total_size / (1024**3)\n",
    "    \n",
    "    print(f\"   Total dataset size: {dataset_info['total_size_gb']:.2f} GB\")\n",
    "    \n",
    "    # Dataset split analysis\n",
    "    print(f\"\\nğŸ“‹ Dataset Split Analysis:\")\n",
    "    for split_name, files in dataset_info['dataset_splits'].items():\n",
    "        if files:\n",
    "            split_size = sum(dataset_info['file_sizes'][f.name] for f in files)\n",
    "            split_size_gb = split_size / (1024**3)\n",
    "            print(f\"   {split_name.capitalize():<8}: {len(files):>3} files ({split_size_gb:.2f} GB)\")\n",
    "            \n",
    "            # Show file pattern\n",
    "            if len(files) <= 3:\n",
    "                for f in files:\n",
    "                    print(f\"     - {f.name}\")\n",
    "            else:\n",
    "                print(f\"     - {files[0].name}\")\n",
    "                print(f\"     - {files[1].name}\")\n",
    "                print(f\"     - ... and {len(files)-2} more files\")\n",
    "    \n",
    "    # File integrity checks\n",
    "    print(f\"\\nğŸ” File Integrity Validation:\")\n",
    "    missing_indices = []\n",
    "    corrupted_files = []\n",
    "    \n",
    "    for tfrecord_file in tfrecord_files:\n",
    "        # Check for corresponding .tfindex file\n",
    "        index_file = tfrecord_file.with_suffix('.tfindex')\n",
    "        if not index_file.exists():\n",
    "            missing_indices.append(tfrecord_file.name)\n",
    "        \n",
    "        # Basic corruption check (file size > 0)\n",
    "        if tfrecord_file.stat().st_size == 0:\n",
    "            corrupted_files.append(tfrecord_file.name)\n",
    "    \n",
    "    if missing_indices:\n",
    "        print(f\"   âš ï¸  Missing .tfindex files: {len(missing_indices)}\")\n",
    "        for missing in missing_indices[:3]:\n",
    "            print(f\"     - {missing}\")\n",
    "    else:\n",
    "        print(f\"   âœ… All TFRecord files have corresponding .tfindex files\")\n",
    "    \n",
    "    if corrupted_files:\n",
    "        print(f\"   âŒ Potentially corrupted files (0 bytes): {len(corrupted_files)}\")\n",
    "        for corrupted in corrupted_files:\n",
    "            print(f\"     - {corrupted}\")\n",
    "    else:\n",
    "        print(f\"   âœ… No obviously corrupted files detected\")\n",
    "    \n",
    "    # Save dataset info for future reference\n",
    "    dataset_info_file = DATA_DIR / \"interim\" / \"dataset_discovery_info.pkl\"\n",
    "    dataset_info_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Convert Path objects to strings for serialization\n",
    "    serializable_info = {\n",
    "        'tfrecord_files': [str(f) for f in dataset_info['tfrecord_files']],\n",
    "        'tfindex_files': [str(f) for f in dataset_info['tfindex_files']],\n",
    "        'file_sizes': dataset_info['file_sizes'],\n",
    "        'dataset_splits': {\n",
    "            'train': [str(f) for f in dataset_info['dataset_splits']['train']],\n",
    "            'eval': [str(f) for f in dataset_info['dataset_splits']['eval']],\n",
    "            'test': [str(f) for f in dataset_info['dataset_splits']['test']]\n",
    "        },\n",
    "        'total_size_gb': dataset_info['total_size_gb'],\n",
    "        'discovery_timestamp': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(dataset_info_file, 'wb') as f:\n",
    "        pickle.dump(serializable_info, f)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Dataset info saved to: {dataset_info_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâŒ TFRecord data directory not found!\")\n",
    "    print(\"ğŸ” Searching for dataset in alternative locations...\")\n",
    "    \n",
    "    # Comprehensive search for the dataset\n",
    "    search_locations = [\n",
    "        PROJECT_ROOT / \"ndws_western_dataset\",\n",
    "        PROJECT_ROOT / \"data\" / \"ndws_western_dataset\", \n",
    "        DATA_DIR / \"ndws_western_dataset\",\n",
    "        DATA_DIR / \"raw\",\n",
    "        PROJECT_ROOT,\n",
    "        Path.cwd()\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nğŸ“ Checking {len(search_locations)} possible locations:\")\n",
    "    found_alternative = False\n",
    "    \n",
    "    for i, loc in enumerate(search_locations, 1):\n",
    "        print(f\"   {i}. {loc}\")\n",
    "        if loc.exists():\n",
    "            # Check if it contains TFRecord files\n",
    "            tfrecord_files_here = list(loc.glob(\"*.tfrecord\"))\n",
    "            if tfrecord_files_here:\n",
    "                print(f\"      âœ… Found {len(tfrecord_files_here)} TFRecord files!\")\n",
    "                TFRECORD_DATA_DIR = loc\n",
    "                found_alternative = True\n",
    "                break\n",
    "            else:\n",
    "                print(f\"      ğŸ“ Directory exists but no TFRecord files found\")\n",
    "        else:\n",
    "            print(f\"      âŒ Directory does not exist\")\n",
    "    \n",
    "    if not found_alternative:\n",
    "        print(f\"\\nğŸš¨ DATASET NOT FOUND!\")\n",
    "        print(f\"ğŸ“‹ Please ensure the wildfire dataset is available in one of these locations:\")\n",
    "        print(f\"   â€¢ {DATA_DIR / 'raw' / 'ndws_western_dataset'}\")\n",
    "        print(f\"   â€¢ {PROJECT_ROOT / 'ndws_western_dataset'}\")\n",
    "        print(f\"\\nğŸ“¥ Dataset download instructions:\")\n",
    "        print(f\"   1. Download the NDWS Western dataset\")\n",
    "        print(f\"   2. Extract to: {DATA_DIR / 'raw' / 'ndws_western_dataset'}\")\n",
    "        print(f\"   3. Verify .tfrecord files are present\")\n",
    "        \n",
    "        # Create placeholder info for debugging\n",
    "        dataset_info = {\n",
    "            'tfrecord_files': [],\n",
    "            'dataset_splits': {'train': [], 'eval': [], 'test': []},\n",
    "            'total_size_gb': 0,\n",
    "            'status': 'not_found'\n",
    "        }\n",
    "\n",
    "# Dataset readiness assessment\n",
    "print(f\"\\nğŸ¯ DATASET READINESS ASSESSMENT\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if dataset_info['tfrecord_files']:\n",
    "    print(f\"âœ… Status: READY FOR PRODUCTION\")\n",
    "    print(f\"ğŸ“Š Files available: {len(dataset_info['tfrecord_files'])}\")\n",
    "    print(f\"ğŸ’¾ Total size: {dataset_info['total_size_gb']:.2f} GB\")\n",
    "    print(f\"ğŸ”„ Splits available: {sum(1 for split in dataset_info['dataset_splits'].values() if split)}\")\n",
    "    \n",
    "    # Estimate processing requirements\n",
    "    estimated_memory_gb = dataset_info['total_size_gb'] * 2  # Conservative estimate\n",
    "    print(f\"\\nğŸ’¡ Processing Estimates:\")\n",
    "    print(f\"   Recommended RAM: {estimated_memory_gb:.1f}+ GB\")\n",
    "    print(f\"   Processing time estimate: {len(dataset_info['tfrecord_files']) * 2}-{len(dataset_info['tfrecord_files']) * 5} minutes\")\n",
    "    print(f\"   GPU acceleration: {'Recommended' if dataset_info['total_size_gb'] > 1 else 'Optional'}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Status: NOT READY\")\n",
    "    print(f\"ğŸš¨ Action required: Download and setup dataset\")\n",
    "    \n",
    "print(f\"\\nğŸš€ Ready to proceed to data parsing and analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c518d",
   "metadata": {},
   "source": [
    "## 6. Parse TFRecord Files\n",
    "\n",
    "Write utility script to parse TFRecord files and extract sample records using tensorflow's tf.data.TFRecordDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d611867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting production-scale dataset analysis...\n",
      "ğŸ” COMPREHENSIVE DATASET ANALYSIS\n",
      "==================================================\n",
      "Analyzing 54 files...\n",
      "Analyzing all samples per file\n",
      "\n",
      "ğŸ“ Analyzing file 1: cleaned_eval_ndws_conus_western_000.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 252 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 2: cleaned_eval_ndws_conus_western_001.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 106 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 3: cleaned_eval_ndws_conus_western_002.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 64 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 4: cleaned_eval_ndws_conus_western_003.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 134 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 5: cleaned_eval_ndws_conus_western_004.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 568 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 6: cleaned_eval_ndws_conus_western_005.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 277 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 7: cleaned_eval_ndws_conus_western_006.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 817 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 8: cleaned_eval_ndws_conus_western_007.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 253 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 9: cleaned_eval_ndws_conus_western_008.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 261 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 10: cleaned_eval_ndws_conus_western_009.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 384 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 11: cleaned_eval_ndws_conus_western_010.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 420 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 12: cleaned_eval_ndws_conus_western_011.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 175 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 13: cleaned_eval_ndws_conus_western_012.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 590 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 14: cleaned_test_ndws_conus_western_000.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 507 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 15: cleaned_test_ndws_conus_western_001.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 365 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 16: cleaned_test_ndws_conus_western_002.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 444 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 17: cleaned_test_ndws_conus_western_003.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 478 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 18: cleaned_test_ndws_conus_western_004.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 205 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 19: cleaned_test_ndws_conus_western_005.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 354 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 20: cleaned_test_ndws_conus_western_006.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 178 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 21: cleaned_test_ndws_conus_western_008.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 246 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 22: cleaned_test_ndws_conus_western_009.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 147 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 23: cleaned_test_ndws_conus_western_010.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 385 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 24: cleaned_test_ndws_conus_western_011.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 607 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 25: cleaned_test_ndws_conus_western_012.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 897 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 26: cleaned_test_ndws_conus_western_013.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 317 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 27: cleaned_test_ndws_conus_western_014.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 436 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 28: cleaned_test_ndws_conus_western_015.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 176 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 29: cleaned_test_ndws_conus_western_016.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 454 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 30: cleaned_train_ndws_conus_western_000.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 350 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 31: cleaned_train_ndws_conus_western_001.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 693 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 32: cleaned_train_ndws_conus_western_002.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 248 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 33: cleaned_train_ndws_conus_western_003.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 657 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 34: cleaned_train_ndws_conus_western_004.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 248 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 35: cleaned_train_ndws_conus_western_005.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 287 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 36: cleaned_train_ndws_conus_western_006.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 114 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 37: cleaned_train_ndws_conus_western_007.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 403 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 38: cleaned_train_ndws_conus_western_008.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 701 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 39: cleaned_train_ndws_conus_western_009.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 353 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 40: cleaned_train_ndws_conus_western_010.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 369 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 41: cleaned_train_ndws_conus_western_011.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 280 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 42: cleaned_train_ndws_conus_western_012.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 241 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 43: cleaned_train_ndws_conus_western_013.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 768 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 44: cleaned_train_ndws_conus_western_014.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 187 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 45: cleaned_train_ndws_conus_western_015.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 213 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 46: cleaned_train_ndws_conus_western_016.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 821 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 47: cleaned_train_ndws_conus_western_017.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 477 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 48: cleaned_train_ndws_conus_western_018.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 193 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 49: cleaned_train_ndws_conus_western_019.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 260 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 50: cleaned_train_ndws_conus_western_020.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 392 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 51: cleaned_train_ndws_conus_western_021.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 443 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 52: cleaned_train_ndws_conus_western_022.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 386 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 53: cleaned_train_ndws_conus_western_023.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 286 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 54: cleaned_train_ndws_conus_western_024.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 230 samples, 23 features\n",
      "\n",
      "ğŸ“‹ ANALYSIS RESULTS SUMMARY\n",
      "========================================\n",
      "âœ… Total samples analyzed: 20097\n",
      "ğŸ”§ Unique features discovered: 23\n",
      "âš ï¸  Total parsing errors: 0\n",
      "\n",
      "ğŸ” FEATURE CONSISTENCY ANALYSIS\n",
      "------------------------------\n",
      "âœ… wdir_wind:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-4.535, 4.338]\n",
      "\n",
      "âœ… fuel1:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-8.301, 11.521]\n",
      "\n",
      "âœ… bi:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-1029.815, 210.062]\n",
      "\n",
      "âœ… viirs_FireMask:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 1.000]\n",
      "\n",
      "âœ… tmp_75:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-9.443, 44.469]\n",
      "\n",
      "âœ… elevation:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-21.000, 4279.000]\n",
      "\n",
      "âœ… erc:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-842.218, 190.920]\n",
      "\n",
      "âœ… chili:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 254.000]\n",
      "\n",
      "âœ… viirs_PrevFireMask:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 1.000]\n",
      "\n",
      "âœ… tmp_day:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-9.587, 44.437]\n",
      "\n",
      "âœ… NDVI:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-2000.000, 9987.000]\n",
      "\n",
      "âœ… pdsi:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-74.362, 118.126]\n",
      "\n",
      "âœ… wind_avg:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-0.282, 19.328]\n",
      "\n",
      "âœ… fuel2:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-4.750, 12.254]\n",
      "\n",
      "âœ… fuel3:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-37.611, 3.533]\n",
      "\n",
      "âœ… impervious:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 100.000]\n",
      "\n",
      "âœ… population:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 18564.754]\n",
      "\n",
      "âœ… gust_med:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-0.345, 27.641]\n",
      "\n",
      "âœ… pr:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-8.174, 92.292]\n",
      "\n",
      "âœ… avg_sph:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 0.014]\n",
      "\n",
      "âœ… wdir_gust:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-4.686, 4.370]\n",
      "\n",
      "âœ… wind_75:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-0.352, 21.705]\n",
      "\n",
      "âœ… water:\n",
      "    Shapes: ['[4096]']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 100.000]\n",
      "\n",
      "ğŸ’¾ Comprehensive analysis saved to: c:\\Users\\Harshvardhan\\OneDrive\\Desktop\\wildfire_pred\\data\\interim\\comprehensive_feature_analysis.pkl\n",
      "ğŸ“„ Summary report saved to: c:\\Users\\Harshvardhan\\OneDrive\\Desktop\\wildfire_pred\\data\\processed\\dataset_exploration_report.txt\n",
      "\n",
      "âœ… Production-scale TFRecord analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# PRODUCTION-SCALE TFRECORD PARSING WITH ERROR HANDLING\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def parse_tfrecord_example_robust(example_proto):\n",
    "    \"\"\"\n",
    "    Robust TFRecord parsing with comprehensive error handling.\n",
    "    Handles mixed data types and serialization formats.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the serialized example\n",
    "        parsed_example = tf.train.Example.FromString(example_proto.numpy())\n",
    "        \n",
    "        feature_dict = {}\n",
    "        parsing_errors = []\n",
    "        \n",
    "        for feature_name, feature in parsed_example.features.feature.items():\n",
    "            try:\n",
    "                if feature.HasField('bytes_list'):\n",
    "                    # Handle tensor data stored as bytes\n",
    "                    bytes_data = feature.bytes_list.value[0]\n",
    "                    \n",
    "                    # Try multiple parsing strategies\n",
    "                    parsed_successfully = False\n",
    "                    \n",
    "                    # Strategy 1: Parse as tensor\n",
    "                    for dtype in [tf.float32, tf.int32, tf.int64]:\n",
    "                        try:\n",
    "                            decoded = tf.io.parse_tensor(bytes_data, dtype)\n",
    "                            feature_dict[feature_name] = {\n",
    "                                'type': f'tensor_{dtype.name}',\n",
    "                                'shape': decoded.shape.as_list(),\n",
    "                                'data': decoded.numpy(),\n",
    "                                'parsing_method': 'tensor'\n",
    "                            }\n",
    "                            parsed_successfully = True\n",
    "                            break\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    # Strategy 2: Raw bytes if tensor parsing fails\n",
    "                    if not parsed_successfully:\n",
    "                        feature_dict[feature_name] = {\n",
    "                            'type': 'bytes',\n",
    "                            'shape': [len(bytes_data)],\n",
    "                            'data': bytes_data,\n",
    "                            'parsing_method': 'raw_bytes'\n",
    "                        }\n",
    "                        \n",
    "                elif feature.HasField('float_list'):\n",
    "                    values = list(feature.float_list.value)\n",
    "                    feature_dict[feature_name] = {\n",
    "                        'type': 'float_list',\n",
    "                        'shape': [len(values)],\n",
    "                        'data': np.array(values),\n",
    "                        'parsing_method': 'direct'\n",
    "                    }\n",
    "                    \n",
    "                elif feature.HasField('int64_list'):\n",
    "                    values = list(feature.int64_list.value)\n",
    "                    feature_dict[feature_name] = {\n",
    "                        'type': 'int64_list',\n",
    "                        'shape': [len(values)],\n",
    "                        'data': np.array(values),\n",
    "                        'parsing_method': 'direct'\n",
    "                    }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                parsing_errors.append(f\"{feature_name}: {str(e)}\")\n",
    "                # Add placeholder for failed features\n",
    "                feature_dict[feature_name] = {\n",
    "                    'type': 'parse_error',\n",
    "                    'shape': 'unknown',\n",
    "                    'data': None,\n",
    "                    'error': str(e),\n",
    "                    'parsing_method': 'failed'\n",
    "                }\n",
    "        \n",
    "        feature_dict['_parsing_errors'] = parsing_errors\n",
    "        return feature_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'_global_error': str(e)}\n",
    "\n",
    "def analyze_dataset_comprehensively(tfrecord_files, max_files=None, samples_per_file=None):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis across multiple files and samples.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” COMPREHENSIVE DATASET ANALYSIS\")\n",
    "    print(f\"=\" * 50)\n",
    "    files_to_analyze = len(tfrecord_files) if max_files is None else min(len(tfrecord_files), max_files)\n",
    "    print(f\"Analyzing {files_to_analyze} files...\")\n",
    "    if samples_per_file is not None:\n",
    "        print(f\"Samples per file: {samples_per_file}\")\n",
    "    else:\n",
    "        print(f\"Analyzing all samples per file\")\n",
    "    \n",
    "    all_features = {}\n",
    "    file_analysis = {}\n",
    "    global_stats = {\n",
    "        'total_samples_analyzed': 0,\n",
    "        'total_parsing_errors': 0,\n",
    "        'unique_features': set(),\n",
    "        'consistent_shapes': {},\n",
    "        'data_type_distribution': {}\n",
    "    }\n",
    "    \n",
    "    files_to_process = tfrecord_files if max_files is None else tfrecord_files[:max_files]\n",
    "    \n",
    "    for file_idx, file_path in enumerate(files_to_process):\n",
    "        print(f\"\\nğŸ“ Analyzing file {file_idx + 1}: {file_path.name}\")\n",
    "        \n",
    "        file_stats = {\n",
    "            'samples_processed': 0,\n",
    "            'parsing_errors': 0,\n",
    "            'features_found': set(),\n",
    "            'file_size_mb': file_path.stat().st_size / (1024 * 1024)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            dataset = tf.data.TFRecordDataset(str(file_path))\n",
    "            \n",
    "            # Process all samples if samples_per_file is None, otherwise limit\n",
    "            dataset_to_process = dataset if samples_per_file is None else dataset.take(samples_per_file)\n",
    "            \n",
    "            for sample_idx, example in enumerate(dataset_to_process):\n",
    "                parsed = parse_tfrecord_example_robust(example)\n",
    "                file_stats['samples_processed'] += 1\n",
    "                global_stats['total_samples_analyzed'] += 1\n",
    "                \n",
    "                # Check for parsing errors\n",
    "                if '_parsing_errors' in parsed and parsed['_parsing_errors']:\n",
    "                    file_stats['parsing_errors'] += len(parsed['_parsing_errors'])\n",
    "                    global_stats['total_parsing_errors'] += len(parsed['_parsing_errors'])\n",
    "                \n",
    "                if '_global_error' in parsed:\n",
    "                    file_stats['parsing_errors'] += 1\n",
    "                    global_stats['total_parsing_errors'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Analyze each feature\n",
    "                for feature_name, info in parsed.items():\n",
    "                    if feature_name.startswith('_'):\n",
    "                        continue\n",
    "                        \n",
    "                    file_stats['features_found'].add(feature_name)\n",
    "                    global_stats['unique_features'].add(feature_name)\n",
    "                    \n",
    "                    if feature_name not in all_features:\n",
    "                        all_features[feature_name] = {\n",
    "                            'shapes': [],\n",
    "                            'types': [],\n",
    "                            'parsing_methods': [],\n",
    "                            'sample_data': [],\n",
    "                            'files_found_in': [],\n",
    "                            'statistics': {\n",
    "                                'min_vals': [],\n",
    "                                'max_vals': [],\n",
    "                                'mean_vals': [],\n",
    "                                'nan_counts': [],\n",
    "                                'inf_counts': []\n",
    "                            }\n",
    "                        }\n",
    "                    \n",
    "                    # Collect feature information\n",
    "                    feature_info = all_features[feature_name]\n",
    "                    feature_info['shapes'].append(info.get('shape', 'unknown'))\n",
    "                    feature_info['types'].append(info.get('type', 'unknown'))\n",
    "                    feature_info['parsing_methods'].append(info.get('parsing_method', 'unknown'))\n",
    "                    feature_info['files_found_in'].append(file_path.name)\n",
    "                    \n",
    "                    # Statistical analysis for numeric data\n",
    "                    if info.get('data') is not None and isinstance(info['data'], np.ndarray):\n",
    "                        data = info['data']\n",
    "                        if data.size > 0 and np.issubdtype(data.dtype, np.number):\n",
    "                            # Calculate statistics on finite values\n",
    "                            finite_data = data[np.isfinite(data)]\n",
    "                            if len(finite_data) > 0:\n",
    "                                feature_info['statistics']['min_vals'].append(float(np.min(finite_data)))\n",
    "                                feature_info['statistics']['max_vals'].append(float(np.max(finite_data)))\n",
    "                                feature_info['statistics']['mean_vals'].append(float(np.mean(finite_data)))\n",
    "                            \n",
    "                            # Count problematic values\n",
    "                            nan_count = int(np.sum(np.isnan(data)))\n",
    "                            inf_count = int(np.sum(np.isinf(data)))\n",
    "                            feature_info['statistics']['nan_counts'].append(nan_count)\n",
    "                            feature_info['statistics']['inf_counts'].append(inf_count)\n",
    "                    \n",
    "                    # Store sample data for the first occurrence\n",
    "                    if len(feature_info['sample_data']) < 3:\n",
    "                        if info.get('data') is not None:\n",
    "                            if isinstance(info['data'], np.ndarray) and info['data'].size <= 100:\n",
    "                                feature_info['sample_data'].append(info['data'])\n",
    "                \n",
    "                # Progress indicator\n",
    "                if sample_idx == 0:\n",
    "                    print(f\"  âœ“ Sample {sample_idx + 1}: {len(parsed)} features parsed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error processing {file_path.name}: {e}\")\n",
    "            file_stats['file_error'] = str(e)\n",
    "        \n",
    "        file_analysis[file_path.name] = file_stats\n",
    "        print(f\"  ğŸ“Š File summary: {file_stats['samples_processed']} samples, {len(file_stats['features_found'])} features\")\n",
    "        \n",
    "        if file_stats['parsing_errors'] > 0:\n",
    "            print(f\"  âš ï¸  Parsing errors: {file_stats['parsing_errors']}\")\n",
    "    \n",
    "    return all_features, file_analysis, global_stats\n",
    "\n",
    "# Execute comprehensive analysis if files are available\n",
    "if 'dataset_info' in locals() and dataset_info['tfrecord_files']:\n",
    "    print(\"ğŸš€ Starting production-scale dataset analysis...\")\n",
    "\n",
    "    # Use the TFRecord files from our dataset discovery\n",
    "    available_files = [Path(f) for f in dataset_info['tfrecord_files']] if isinstance(dataset_info['tfrecord_files'][0], str) else dataset_info['tfrecord_files']\n",
    "    \n",
    "    # Analyze ALL files and ALL samples for comprehensive analysis\n",
    "    feature_analysis, file_analysis, global_stats = analyze_dataset_comprehensively(\n",
    "        available_files, \n",
    "        max_files=None,  # Analyze ALL files\n",
    "        samples_per_file=None  # Analyze ALL samples per file\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ANALYSIS RESULTS SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"âœ… Total samples analyzed: {global_stats['total_samples_analyzed']}\")\n",
    "    print(f\"ğŸ”§ Unique features discovered: {len(global_stats['unique_features'])}\")\n",
    "    print(f\"âš ï¸  Total parsing errors: {global_stats['total_parsing_errors']}\")\n",
    "    \n",
    "    # Feature consistency analysis\n",
    "    print(f\"\\nğŸ” FEATURE CONSISTENCY ANALYSIS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for feature_name, info in feature_analysis.items():\n",
    "        unique_shapes = set(str(shape) for shape in info['shapes'])\n",
    "        unique_types = set(info['types'])\n",
    "        \n",
    "        consistency_status = \"âœ…\" if len(unique_shapes) == 1 and len(unique_types) == 1 else \"âš ï¸ \"\n",
    "        \n",
    "        print(f\"{consistency_status} {feature_name}:\")\n",
    "        print(f\"    Shapes: {list(unique_shapes)}\")\n",
    "        print(f\"    Types: {list(unique_types)}\")\n",
    "        print(f\"    Found in: {len(set(info['files_found_in']))} file(s)\")\n",
    "        \n",
    "        # Value range information if available\n",
    "        if info['statistics']['min_vals'] and info['statistics']['max_vals']:\n",
    "            min_val = min(info['statistics']['min_vals'])\n",
    "            max_val = max(info['statistics']['max_vals'])\n",
    "            print(f\"    Value range: [{min_val:.3f}, {max_val:.3f}]\")\n",
    "        \n",
    "        # Data quality issues\n",
    "        total_nans = sum(info['statistics'].get('nan_counts', [0]))\n",
    "        total_infs = sum(info['statistics'].get('inf_counts', [0]))\n",
    "        \n",
    "        if total_nans > 0 or total_infs > 0:\n",
    "            print(f\"    âš ï¸  Quality issues: {total_nans} NaNs, {total_infs} Infs\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Save comprehensive analysis results\n",
    "    analysis_results_file = DATA_DIR / \"interim\" / \"comprehensive_feature_analysis.pkl\"\n",
    "    \n",
    "    with open(analysis_results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'feature_analysis': feature_analysis,\n",
    "            'file_analysis': file_analysis,\n",
    "            'global_stats': global_stats,\n",
    "            'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'analysis_parameters': {\n",
    "                'max_files_analyzed': len(available_files),\n",
    "                'samples_per_file': 'all',\n",
    "                'total_files_available': len(available_files)\n",
    "            }\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Comprehensive analysis saved to: {analysis_results_file}\")\n",
    "    \n",
    "    # Generate summary report\n",
    "    summary_report_path = DATA_DIR / \"processed\" / \"dataset_exploration_report.txt\"\n",
    "    summary_report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(summary_report_path, 'w') as f:\n",
    "        f.write(\"WILDFIRE DATASET EXPLORATION REPORT\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        f.write(f\"Analysis Date: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(f\"Dataset Location: {TFRECORD_DATA_DIR}\\n\")\n",
    "        f.write(f\"Total Files Available: {len(available_files)}\\n\")\n",
    "        f.write(f\"Files Analyzed: {len(available_files)}\\n\")\n",
    "        f.write(f\"Total Samples Analyzed: {global_stats['total_samples_analyzed']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"FEATURE SUMMARY:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for feature_name, info in feature_analysis.items():\n",
    "            f.write(f\"{feature_name}:\\n\")\n",
    "            f.write(f\"  - Shapes: {set(str(s) for s in info['shapes'])}\\n\")\n",
    "            f.write(f\"  - Types: {set(info['types'])}\\n\")\n",
    "            f.write(f\"  - Found in {len(set(info['files_found_in']))} files\\n\")\n",
    "            \n",
    "            # Statistical summary\n",
    "            if info['statistics']['min_vals']:\n",
    "                min_val = min(info['statistics']['min_vals'])\n",
    "                max_val = max(info['statistics']['max_vals'])\n",
    "                mean_val = np.mean(info['statistics']['mean_vals'])\n",
    "                f.write(f\"  - Range: [{min_val:.4f}, {max_val:.3f}], Mean: {mean_val:.3f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"ğŸ“„ Summary report saved to: {summary_report_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No TFRecord files found for analysis\")\n",
    "\n",
    "print(f\"\\nâœ… Production-scale TFRecord analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218e60b",
   "metadata": {},
   "source": [
    "## 7. Feature Names and Tensor Analysis\n",
    "\n",
    "Identify and analyze feature names from the dataset, understand tensor dimensions and examine sample data values to understand the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6de58764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting production-scale dataset analysis...\n",
      "ğŸ” COMPREHENSIVE DATASET ANALYSIS\n",
      "==================================================\n",
      "Analyzing 54 files...\n",
      "Analyzing all samples per file\n",
      "\n",
      "ğŸ“ Analyzing file 1: cleaned_eval_ndws_conus_western_000.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 252 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 2: cleaned_eval_ndws_conus_western_001.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 106 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 3: cleaned_eval_ndws_conus_western_002.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 64 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 4: cleaned_eval_ndws_conus_western_003.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 134 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 5: cleaned_eval_ndws_conus_western_004.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 568 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 6: cleaned_eval_ndws_conus_western_005.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 277 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 7: cleaned_eval_ndws_conus_western_006.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 817 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 8: cleaned_eval_ndws_conus_western_007.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 253 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 9: cleaned_eval_ndws_conus_western_008.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 261 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 10: cleaned_eval_ndws_conus_western_009.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 384 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 11: cleaned_eval_ndws_conus_western_010.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 420 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 12: cleaned_eval_ndws_conus_western_011.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 175 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 13: cleaned_eval_ndws_conus_western_012.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 590 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 14: cleaned_test_ndws_conus_western_000.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 507 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 15: cleaned_test_ndws_conus_western_001.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 365 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 16: cleaned_test_ndws_conus_western_002.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 444 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 17: cleaned_test_ndws_conus_western_003.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 478 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 18: cleaned_test_ndws_conus_western_004.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 205 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 19: cleaned_test_ndws_conus_western_005.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 354 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 20: cleaned_test_ndws_conus_western_006.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 178 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 21: cleaned_test_ndws_conus_western_008.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 246 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 22: cleaned_test_ndws_conus_western_009.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 147 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 23: cleaned_test_ndws_conus_western_010.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 385 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 24: cleaned_test_ndws_conus_western_011.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 607 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 25: cleaned_test_ndws_conus_western_012.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 897 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 26: cleaned_test_ndws_conus_western_013.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 317 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 27: cleaned_test_ndws_conus_western_014.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 436 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 28: cleaned_test_ndws_conus_western_015.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 176 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 29: cleaned_test_ndws_conus_western_016.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 454 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 30: cleaned_train_ndws_conus_western_000.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 350 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 31: cleaned_train_ndws_conus_western_001.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 693 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 32: cleaned_train_ndws_conus_western_002.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 248 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 33: cleaned_train_ndws_conus_western_003.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 657 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 34: cleaned_train_ndws_conus_western_004.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 248 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 35: cleaned_train_ndws_conus_western_005.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 287 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 36: cleaned_train_ndws_conus_western_006.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 114 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 37: cleaned_train_ndws_conus_western_007.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 403 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 38: cleaned_train_ndws_conus_western_008.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 701 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 39: cleaned_train_ndws_conus_western_009.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 353 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 40: cleaned_train_ndws_conus_western_010.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 369 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 41: cleaned_train_ndws_conus_western_011.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 280 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 42: cleaned_train_ndws_conus_western_012.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 241 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 43: cleaned_train_ndws_conus_western_013.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 768 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 44: cleaned_train_ndws_conus_western_014.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 187 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 45: cleaned_train_ndws_conus_western_015.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 213 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 46: cleaned_train_ndws_conus_western_016.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 821 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 47: cleaned_train_ndws_conus_western_017.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 477 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 48: cleaned_train_ndws_conus_western_018.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 193 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 49: cleaned_train_ndws_conus_western_019.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 260 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 50: cleaned_train_ndws_conus_western_020.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 392 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 51: cleaned_train_ndws_conus_western_021.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 443 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 52: cleaned_train_ndws_conus_western_022.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 386 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 53: cleaned_train_ndws_conus_western_023.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 286 samples, 23 features\n",
      "\n",
      "ğŸ“ Analyzing file 54: cleaned_train_ndws_conus_western_024.tfrecord\n",
      "  âœ“ Sample 1: 24 features parsed\n",
      "  ğŸ“Š File summary: 230 samples, 23 features\n",
      "\n",
      "ğŸ“‹ ANALYSIS RESULTS SUMMARY\n",
      "========================================\n",
      "âœ… Total samples analyzed: 20097\n",
      "ğŸ”§ Unique features discovered: 23\n",
      "âš ï¸  Total parsing errors: 0\n",
      "\n",
      "ğŸ” FEATURE CONSISTENCY ANALYSIS\n",
      "------------------------------\n",
      "âœ… wdir_wind:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-4.535, 4.338]\n",
      "\n",
      "âœ… fuel1:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-8.301, 11.521]\n",
      "\n",
      "âœ… bi:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-1029.815, 210.062]\n",
      "\n",
      "âœ… viirs_FireMask:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 1.000]\n",
      "\n",
      "âœ… tmp_75:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-9.443, 44.469]\n",
      "\n",
      "âœ… elevation:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-21.000, 4279.000]\n",
      "\n",
      "âœ… erc:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-842.218, 190.920]\n",
      "\n",
      "âœ… chili:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 254.000]\n",
      "\n",
      "âœ… viirs_PrevFireMask:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 1.000]\n",
      "\n",
      "âœ… tmp_day:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-9.587, 44.437]\n",
      "\n",
      "âœ… NDVI:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-2000.000, 9987.000]\n",
      "\n",
      "âœ… pdsi:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-74.362, 118.126]\n",
      "\n",
      "âœ… wind_avg:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-0.282, 19.328]\n",
      "\n",
      "âœ… fuel2:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-4.750, 12.254]\n",
      "\n",
      "âœ… fuel3:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-37.611, 3.533]\n",
      "\n",
      "âœ… impervious:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 100.000]\n",
      "\n",
      "âœ… population:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 18564.754]\n",
      "\n",
      "âœ… gust_med:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-0.345, 27.641]\n",
      "\n",
      "âœ… pr:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-8.174, 92.292]\n",
      "\n",
      "âœ… avg_sph:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 0.014]\n",
      "\n",
      "âœ… wdir_gust:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-4.686, 4.370]\n",
      "\n",
      "âœ… wind_75:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [-0.352, 21.705]\n",
      "\n",
      "âœ… water:\n",
      "    Shapes: ['(64, 64)']\n",
      "    Types: ['float_list']\n",
      "    Found in: 54 file(s)\n",
      "    Value range: [0.000, 100.000]\n",
      "\n",
      "ğŸ’¾ Comprehensive analysis saved to: c:\\Users\\Harshvardhan\\OneDrive\\Desktop\\wildfire_pred\\data\\interim\\comprehensive_feature_analysis.pkl\n",
      "ğŸ“„ Summary report saved to: c:\\Users\\Harshvardhan\\OneDrive\\Desktop\\wildfire_pred\\data\\processed\\dataset_exploration_report.txt\n",
      "Analyzing data structure across multiple examples...\n",
      "\n",
      "ğŸ“Š TFRecord Structure Analysis:\n",
      "------------------------------------------------------------\n",
      "           Feature       Type    Shape  Shape_Consistent Min_Value Max_Value Mean_Value\n",
      "         wdir_wind float_list (64, 64)              True       N/A       N/A        N/A\n",
      "             fuel1 float_list (64, 64)              True       N/A       N/A        N/A\n",
      "                bi float_list (64, 64)              True       N/A       N/A        N/A\n",
      "    viirs_FireMask float_list (64, 64)              True       N/A       N/A        N/A\n",
      "            tmp_75 float_list (64, 64)              True       N/A       N/A        N/A\n",
      "         elevation float_list (64, 64)              True       N/A       N/A        N/A\n",
      "               erc float_list (64, 64)              True       N/A       N/A        N/A\n",
      "             chili float_list (64, 64)              True       N/A       N/A        N/A\n",
      "viirs_PrevFireMask float_list (64, 64)              True       N/A       N/A        N/A\n",
      "           tmp_day float_list (64, 64)              True       N/A       N/A        N/A\n",
      "              NDVI float_list (64, 64)              True       N/A       N/A        N/A\n",
      "              pdsi float_list (64, 64)              True       N/A       N/A        N/A\n",
      "          wind_avg float_list (64, 64)              True       N/A       N/A        N/A\n",
      "             fuel2 float_list (64, 64)              True       N/A       N/A        N/A\n",
      "             fuel3 float_list (64, 64)              True       N/A       N/A        N/A\n",
      "        impervious float_list (64, 64)              True       N/A       N/A        N/A\n",
      "        population float_list (64, 64)              True       N/A       N/A        N/A\n",
      "          gust_med float_list (64, 64)              True       N/A       N/A        N/A\n",
      "                pr float_list (64, 64)              True       N/A       N/A        N/A\n",
      "           avg_sph float_list (64, 64)              True       N/A       N/A        N/A\n",
      "         wdir_gust float_list (64, 64)              True       N/A       N/A        N/A\n",
      "           wind_75 float_list (64, 64)              True       N/A       N/A        N/A\n",
      "             water float_list (64, 64)              True       N/A       N/A        N/A\n",
      "\n",
      "âœ… All features have consistent shapes across examples\n",
      "\n",
      "ğŸ“‹ Key features identified:\n",
      "  - elevation\n",
      "  - tmmn\n",
      "  - tmmx\n",
      "  - pr\n",
      "  - fm100\n",
      "  - fm1000\n",
      "  - erc\n",
      "  - PrevFireMask\n",
      "  - FireMask\n",
      "\n",
      "ğŸ¯ COMPREHENSIVE FEATURE ANALYSIS & RECOMMENDATIONS\n",
      "============================================================\n",
      "ğŸ”§ GENERATING PREPROCESSING RECOMMENDATIONS\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š FEATURE CATEGORIZATION\n",
      "------------------------------\n",
      "\n",
      "ğŸ·ï¸  UNKNOWN:\n",
      "  Medium: 16 features\n",
      "    â€¢ wdir_wind\n",
      "    â€¢ fuel1\n",
      "    â€¢ bi\n",
      "    â€¢ ... and 13 more\n",
      "\n",
      "ğŸ·ï¸  FIRE_DETECTION:\n",
      "  Very_High: 1 features\n",
      "    â€¢ viirs_FireMask\n",
      "\n",
      "ğŸ·ï¸  TOPOGRAPHIC:\n",
      "  High: 1 features\n",
      "    â€¢ elevation\n",
      "\n",
      "ğŸ·ï¸  FIRE_WEATHER:\n",
      "  Very_High: 1 features\n",
      "    â€¢ erc\n",
      "\n",
      "ğŸ·ï¸  LAND_COVER:\n",
      "  Low: 2 features\n",
      "    â€¢ impervious\n",
      "    â€¢ water\n",
      "\n",
      "ğŸ·ï¸  ANTHROPOGENIC:\n",
      "  Medium: 1 features\n",
      "    â€¢ population\n",
      "\n",
      "ğŸ·ï¸  METEOROLOGICAL:\n",
      "  Very_High: 1 features\n",
      "    â€¢ pr\n",
      "\n",
      "ğŸ”§ PREPROCESSING RECOMMENDATIONS\n",
      "----------------------------------------\n",
      "ğŸ“ˆ Normalization Strategies:\n",
      "  â€¢ Standard Scaling: 21 features\n",
      "  â€¢ None: 1 features\n",
      "    - viirs_FireMask\n",
      "  â€¢ Log Normalize: 1 features\n",
      "    - population\n",
      "\n",
      "âœ… Comprehensive feature analysis and recommendations complete!\n",
      "ğŸ“Š Analysis covers 23 features across 20097 samples\n"
     ]
    }
   ],
   "source": [
    "# PRODUCTION-SCALE FEATURE CLASSIFICATION AND RECOMMENDATIONS\n",
    "\n",
    "def create_comprehensive_feature_descriptions():\n",
    "    \"\"\"\n",
    "    Create comprehensive descriptions for wildfire dataset features.\n",
    "    Includes data types, expected ranges, and usage recommendations.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'elevation': {\n",
    "            'description': 'Elevation above sea level (meters)',\n",
    "            'expected_range': (0, 5000),\n",
    "            'data_type': 'continuous',\n",
    "            'category': 'topographic',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'high'\n",
    "        },\n",
    "        'th': {\n",
    "            'description': 'Wind direction (degrees from north)', \n",
    "            'expected_range': (0, 360),\n",
    "            'data_type': 'circular',\n",
    "            'category': 'meteorological',\n",
    "            'preprocessing': 'circular_encoding',\n",
    "            'importance': 'medium'\n",
    "        },\n",
    "        'vs': {\n",
    "            'description': 'Wind speed (m/s)',\n",
    "            'expected_range': (0, 50),\n",
    "            'data_type': 'continuous',\n",
    "            'category': 'meteorological',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'high'\n",
    "        },\n",
    "        'tmmn': {\n",
    "            'description': 'Minimum air temperature (Kelvin)',\n",
    "            'expected_range': (200, 330),\n",
    "            'data_type': 'continuous',\n",
    "            'category': 'meteorological',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'very_high'\n",
    "        },\n",
    "        'tmmx': {\n",
    "            'description': 'Maximum air temperature (Kelvin)',\n",
    "            'expected_range': (200, 330),\n",
    "            'data_type': 'continuous',\n",
    "            'category': 'meteorological',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'very_high'\n",
    "        },\n",
    "        'sph': {\n",
    "            'description': 'Specific humidity (kg/kg)',\n",
    "            'expected_range': (0, 0.03),\n",
    "            'data_type': 'continuous',\n",
    "            'category': 'meteorological',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'high'\n",
    "        },\n",
    "        'pr': {\n",
    "            'description': 'Precipitation amount (mm)',\n",
    "            'expected_range': (0, 300),\n",
    "            'data_type': 'continuous',\n",
    "            'category': 'meteorological',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'very_high'\n",
    "        },\n",
    "        'rmax': {\n",
    "            'description': 'Maximum relative humidity (%)',\n",
    "            'expected_range': (0, 100),\n",
    "            'data_type': 'percentage',\n",
    "            'category': 'meteorological',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'high'\n",
    "        },\n",
    "        'rmin': {\n",
    "            'description': 'Minimum relative humidity (%)',\n",
    "            'expected_range': (0, 100),\n",
    "            'data_type': 'percentage',\n",
    "            'category': 'meteorological',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'high'\n",
    "        },\n",
    "        'fm100': {\n",
    "            'description': '100-hour fuel moisture (%)',\n",
    "            'expected_range': (0, 50),\n",
    "            'data_type': 'percentage',\n",
    "            'category': 'fire_weather',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'very_high'\n",
    "        },\n",
    "        'fm1000': {\n",
    "            'description': '1000-hour fuel moisture (%)',\n",
    "            'expected_range': (0, 50),\n",
    "            'data_type': 'percentage',\n",
    "            'category': 'fire_weather',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'very_high'\n",
    "        },\n",
    "        'population': {\n",
    "            'description': 'Population density (people/kmÂ²)',\n",
    "            'expected_range': (0, 10000),\n",
    "            'data_type': 'continuous',\n",
    "            'category': 'anthropogenic',\n",
    "            'preprocessing': 'log_transform',\n",
    "            'importance': 'medium'\n",
    "        },\n",
    "        'erc': {\n",
    "            'description': 'Energy Release Component (fire weather index)',\n",
    "            'expected_range': (0, 200),\n",
    "            'data_type': 'continuous',\n",
    "            'category': 'fire_weather',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'very_high'\n",
    "        },\n",
    "        'PrevFireMask': {\n",
    "            'description': 'Previous day fire occurrence (binary mask)',\n",
    "            'expected_range': (0, 1),\n",
    "            'data_type': 'binary',\n",
    "            'category': 'fire_history',\n",
    "            'preprocessing': 'none',\n",
    "            'importance': 'very_high'\n",
    "        },\n",
    "        'FireMask': {\n",
    "            'description': 'Current day fire occurrence (target variable)',\n",
    "            'expected_range': (0, 1),\n",
    "            'data_type': 'binary',\n",
    "            'category': 'target',\n",
    "            'preprocessing': 'none',\n",
    "            'importance': 'target'\n",
    "        },\n",
    "        'viirs_FireMask': {\n",
    "            'description': 'VIIRS satellite fire detection mask',\n",
    "            'expected_range': (0, 1),\n",
    "            'data_type': 'binary',\n",
    "            'category': 'fire_detection',\n",
    "            'preprocessing': 'none',\n",
    "            'importance': 'very_high'\n",
    "        },\n",
    "        'burned_area': {\n",
    "            'description': 'Area burned (hectares)',\n",
    "            'expected_range': (0, 10000),\n",
    "            'data_type': 'continuous',\n",
    "            'category': 'fire_impact',\n",
    "            'preprocessing': 'log_transform',\n",
    "            'importance': 'medium'\n",
    "        },\n",
    "        'impervious': {\n",
    "            'description': 'Impervious surface percentage',\n",
    "            'expected_range': (0, 100),\n",
    "            'data_type': 'percentage',\n",
    "            'category': 'land_cover',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'low'\n",
    "        },\n",
    "        'water': {\n",
    "            'description': 'Water body percentage',\n",
    "            'expected_range': (0, 100),\n",
    "            'data_type': 'percentage',\n",
    "            'category': 'land_cover',\n",
    "            'preprocessing': 'normalize',\n",
    "            'importance': 'low'\n",
    "        }\n",
    "    }\n",
    "\n",
    "def generate_preprocessing_recommendations(feature_analysis, feature_descriptions):\n",
    "    \"\"\"\n",
    "    Generate specific preprocessing recommendations based on actual data analysis.\n",
    "    \"\"\"\n",
    "    recommendations = {\n",
    "        'normalization_strategies': {},\n",
    "        'outlier_handling': {},\n",
    "        'missing_value_strategies': {},\n",
    "        'feature_engineering_opportunities': [],\n",
    "        'data_quality_issues': [],\n",
    "        'model_input_preparation': {}\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ”§ GENERATING PREPROCESSING RECOMMENDATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for feature_name, analysis_info in feature_analysis.items():\n",
    "        feature_desc = feature_descriptions.get(feature_name, {})\n",
    "        \n",
    "        # Statistical analysis\n",
    "        stats = analysis_info.get('statistics', {})\n",
    "        \n",
    "        # Normalization strategy based on data type and distribution\n",
    "        if feature_desc.get('data_type') == 'circular':\n",
    "            recommendations['normalization_strategies'][feature_name] = 'circular_encoding'\n",
    "        elif feature_desc.get('data_type') == 'binary':\n",
    "            recommendations['normalization_strategies'][feature_name] = 'none'\n",
    "        elif feature_desc.get('preprocessing') == 'log_transform':\n",
    "            recommendations['normalization_strategies'][feature_name] = 'log_normalize'\n",
    "        else:\n",
    "            # Use standard scaling for continuous variables\n",
    "            recommendations['normalization_strategies'][feature_name] = 'standard_scaling'\n",
    "        \n",
    "        # Outlier detection based on expected ranges\n",
    "        expected_range = feature_desc.get('expected_range')\n",
    "        if expected_range and stats.get('min_vals') and stats.get('max_vals'):\n",
    "            min_val = min(stats['min_vals'])\n",
    "            max_val = max(stats['max_vals'])\n",
    "            \n",
    "            outliers_detected = False\n",
    "            if expected_range[0] is not None and min_val < expected_range[0]:\n",
    "                outliers_detected = True\n",
    "            if expected_range[1] is not None and max_val > expected_range[1]:\n",
    "                outliers_detected = True\n",
    "            \n",
    "            if outliers_detected:\n",
    "                recommendations['outlier_handling'][feature_name] = 'clip_to_expected_range'\n",
    "        \n",
    "        # Missing value strategy\n",
    "        nan_count = sum(stats.get('nan_counts', [0]))\n",
    "        inf_count = sum(stats.get('inf_counts', [0]))\n",
    "        \n",
    "        if nan_count > 0 or inf_count > 0:\n",
    "            if feature_desc.get('category') in ['meteorological', 'fire_weather']:\n",
    "                strategy = 'interpolate_temporal'\n",
    "            elif feature_desc.get('data_type') == 'binary':\n",
    "                strategy = 'fill_zeros'\n",
    "            else:\n",
    "                strategy = 'fill_median'\n",
    "            \n",
    "            recommendations['missing_value_strategies'][feature_name] = strategy\n",
    "            recommendations['data_quality_issues'].append({\n",
    "                'feature': feature_name,\n",
    "                'issue': f'{nan_count} NaN values, {inf_count} Inf values',\n",
    "                'severity': 'high' if nan_count + inf_count > 100 else 'medium'\n",
    "            })\n",
    "    \n",
    "    # Feature engineering opportunities\n",
    "    category = feature_desc.get('category', 'unknown')\n",
    "    if category == 'meteorological':\n",
    "        if 'tmmn' in feature_analysis and 'tmmx' in feature_analysis:\n",
    "            recommendations['feature_engineering_opportunities'].append({\n",
    "                'type': 'temperature_range',\n",
    "                'description': 'Create temperature range (tmmx - tmmn) feature',\n",
    "                'features_involved': ['tmmn', 'tmmx']\n",
    "            })\n",
    "        \n",
    "        if 'rmin' in feature_analysis and 'rmax' in feature_analysis:\n",
    "            recommendations['feature_engineering_opportunities'].append({\n",
    "                'type': 'humidity_range',\n",
    "                'description': 'Create humidity range (rmax - rmin) feature',\n",
    "                'features_involved': ['rmin', 'rmax']\n",
    "            })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def parse_tfrecord_example_robust(example):\n",
    "    \"\"\"Robust TFRecord parsing with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        # Parse the example\n",
    "        example_proto = tf.train.Example.FromString(example.numpy())\n",
    "        \n",
    "        feature_dict = {}\n",
    "        parsing_errors = []\n",
    "        \n",
    "        for feature_name, feature in example_proto.features.feature.items():\n",
    "            try:\n",
    "                if feature.HasField('bytes_list'):\n",
    "                    # Try to decode as tensor\n",
    "                    try:\n",
    "                        decoded_tensor = tf.io.parse_tensor(\n",
    "                            feature.bytes_list.value[0], \n",
    "                            tf.float32\n",
    "                        ).numpy()\n",
    "                        \n",
    "                        feature_dict[feature_name] = {\n",
    "                            'data': decoded_tensor,\n",
    "                            'type': 'tensor_float32',\n",
    "                            'shape': decoded_tensor.shape,\n",
    "                            'parsing_method': 'tensor_decode'\n",
    "                        }\n",
    "                    except Exception as e:\n",
    "                        # Fallback to bytes\n",
    "                        feature_dict[feature_name] = {\n",
    "                            'data': feature.bytes_list.value[0],\n",
    "                            'type': 'bytes',\n",
    "                            'shape': len(feature.bytes_list.value),\n",
    "                            'parsing_method': 'bytes_fallback'\n",
    "                        }\n",
    "                        \n",
    "                elif feature.HasField('float_list'):\n",
    "                    float_data = np.array(list(feature.float_list.value), dtype=np.float32)\n",
    "                    \n",
    "                    # Reshape if it looks like image data (4096 = 64x64)\n",
    "                    if len(float_data) == 4096:\n",
    "                        float_data = float_data.reshape(64, 64)\n",
    "                    \n",
    "                    feature_dict[feature_name] = {\n",
    "                        'data': float_data,\n",
    "                        'type': 'float_list',\n",
    "                        'shape': float_data.shape,\n",
    "                        'parsing_method': 'float_array'\n",
    "                    }\n",
    "                    \n",
    "                elif feature.HasField('int64_list'):\n",
    "                    int_data = np.array(list(feature.int64_list.value), dtype=np.int64)\n",
    "                    \n",
    "                    # Reshape if it looks like image data\n",
    "                    if len(int_data) == 4096:\n",
    "                        int_data = int_data.reshape(64, 64)\n",
    "                    \n",
    "                    feature_dict[feature_name] = {\n",
    "                        'data': int_data,\n",
    "                        'type': 'int64_list',\n",
    "                        'shape': int_data.shape,\n",
    "                        'parsing_method': 'int_array'\n",
    "                    }\n",
    "                else:\n",
    "                    parsing_errors.append(f\"Unknown feature type for {feature_name}\")\n",
    "                    feature_dict[feature_name] = {\n",
    "                        'data': None,\n",
    "                        'type': 'unknown',\n",
    "                        'shape': None,\n",
    "                        'parsing_method': 'failed'\n",
    "                    }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                parsing_errors.append(f\"Error parsing {feature_name}: {str(e)}\")\n",
    "                feature_dict[feature_name] = {\n",
    "                    'data': None,\n",
    "                    'type': 'error',\n",
    "                    'shape': None,\n",
    "                    'parsing_method': 'failed'\n",
    "                }\n",
    "        \n",
    "        feature_dict['_parsing_errors'] = parsing_errors\n",
    "        return feature_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'_global_error': str(e)}\n",
    "\n",
    "def analyze_dataset_comprehensively(tfrecord_files, max_files=None, samples_per_file=None):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis across multiple files and samples.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” COMPREHENSIVE DATASET ANALYSIS\")\n",
    "    print(f\"=\" * 50)\n",
    "    files_to_analyze = len(tfrecord_files) if max_files is None else min(len(tfrecord_files), max_files)\n",
    "    print(f\"Analyzing {files_to_analyze} files...\")\n",
    "    if samples_per_file is not None:\n",
    "        print(f\"Samples per file: {samples_per_file}\")\n",
    "    else:\n",
    "        print(f\"Analyzing all samples per file\")\n",
    "    \n",
    "    all_features = {}\n",
    "    file_analysis = {}\n",
    "    global_stats = {\n",
    "        'total_samples_analyzed': 0,\n",
    "        'total_parsing_errors': 0,\n",
    "        'unique_features': set(),\n",
    "        'consistent_shapes': {},\n",
    "        'data_type_distribution': {}\n",
    "    }\n",
    "    \n",
    "    files_to_process = tfrecord_files if max_files is None else tfrecord_files[:max_files]\n",
    "    \n",
    "    for file_idx, file_path in enumerate(files_to_process):\n",
    "        print(f\"\\nğŸ“ Analyzing file {file_idx + 1}: {file_path.name}\")\n",
    "        \n",
    "        file_stats = {\n",
    "            'samples_processed': 0,\n",
    "            'parsing_errors': 0,\n",
    "            'features_found': set(),\n",
    "            'file_size_mb': file_path.stat().st_size / (1024 * 1024)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            dataset = tf.data.TFRecordDataset(str(file_path))\n",
    "            \n",
    "            # Use samples_per_file limitation if specified\n",
    "            dataset_to_process = dataset.take(samples_per_file) if samples_per_file is not None else dataset\n",
    "            \n",
    "            for sample_idx, example in enumerate(dataset_to_process):\n",
    "                parsed = parse_tfrecord_example_robust(example)\n",
    "                file_stats['samples_processed'] += 1\n",
    "                global_stats['total_samples_analyzed'] += 1\n",
    "                \n",
    "                # Check for parsing errors\n",
    "                if '_parsing_errors' in parsed and parsed['_parsing_errors']:\n",
    "                    file_stats['parsing_errors'] += len(parsed['_parsing_errors'])\n",
    "                    global_stats['total_parsing_errors'] += len(parsed['_parsing_errors'])\n",
    "                \n",
    "                if '_global_error' in parsed:\n",
    "                    file_stats['parsing_errors'] += 1\n",
    "                    global_stats['total_parsing_errors'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Analyze each feature\n",
    "                for feature_name, info in parsed.items():\n",
    "                    if feature_name.startswith('_'):\n",
    "                        continue\n",
    "                        \n",
    "                    file_stats['features_found'].add(feature_name)\n",
    "                    global_stats['unique_features'].add(feature_name)\n",
    "                    \n",
    "                    if feature_name not in all_features:\n",
    "                        all_features[feature_name] = {\n",
    "                            'shapes': [],\n",
    "                            'types': [],\n",
    "                            'parsing_methods': [],\n",
    "                            'sample_data': [],\n",
    "                            'files_found_in': [],\n",
    "                            'statistics': {\n",
    "                                'min_vals': [],\n",
    "                                'max_vals': [],\n",
    "                                'mean_vals': [],\n",
    "                                'nan_counts': [],\n",
    "                                'inf_counts': []\n",
    "                            }\n",
    "                        }\n",
    "                    \n",
    "                    # Collect feature information\n",
    "                    feature_info = all_features[feature_name]\n",
    "                    feature_info['shapes'].append(info.get('shape', 'unknown'))\n",
    "                    feature_info['types'].append(info.get('type', 'unknown'))\n",
    "                    feature_info['parsing_methods'].append(info.get('parsing_method', 'unknown'))\n",
    "                    feature_info['files_found_in'].append(file_path.name)\n",
    "                    \n",
    "                    # Statistical analysis for numeric data\n",
    "                    if info.get('data') is not None and isinstance(info['data'], np.ndarray):\n",
    "                        data = info['data']\n",
    "                        if data.size > 0 and np.issubdtype(data.dtype, np.number):\n",
    "                            # Calculate statistics on finite values\n",
    "                            finite_data = data[np.isfinite(data)]\n",
    "                            if len(finite_data) > 0:\n",
    "                                feature_info['statistics']['min_vals'].append(float(np.min(finite_data)))\n",
    "                                feature_info['statistics']['max_vals'].append(float(np.max(finite_data)))\n",
    "                                feature_info['statistics']['mean_vals'].append(float(np.mean(finite_data)))\n",
    "                            \n",
    "                            # Count problematic values\n",
    "                            nan_count = int(np.sum(np.isnan(data)))\n",
    "                            inf_count = int(np.sum(np.isinf(data)))\n",
    "                            feature_info['statistics']['nan_counts'].append(nan_count)\n",
    "                            feature_info['statistics']['inf_counts'].append(inf_count)\n",
    "                \n",
    "                # Progress indicator\n",
    "                if sample_idx == 0:\n",
    "                    print(f\"  âœ“ Sample {sample_idx + 1}: {len(parsed)} features parsed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error processing {file_path.name}: {e}\")\n",
    "            file_stats['file_error'] = str(e)\n",
    "        \n",
    "        file_analysis[file_path.name] = file_stats\n",
    "        print(f\"  ğŸ“Š File summary: {file_stats['samples_processed']} samples, {len(file_stats['features_found'])} features\")\n",
    "        \n",
    "        if file_stats['parsing_errors'] > 0:\n",
    "            print(f\"  âš ï¸  Parsing errors: {file_stats['parsing_errors']}\")\n",
    "    \n",
    "    return all_features, file_analysis, global_stats\n",
    "\n",
    "# Execute comprehensive analysis if files are available\n",
    "if 'dataset_info' in locals() and dataset_info['tfrecord_files']:\n",
    "    print(\"ğŸš€ Starting production-scale dataset analysis...\")\n",
    "    \n",
    "    # Use the TFRecord files from our dataset discovery\n",
    "    available_files = [Path(f) for f in dataset_info['tfrecord_files']] if isinstance(dataset_info['tfrecord_files'][0], str) else dataset_info['tfrecord_files']\n",
    "    \n",
    "    # Perform comprehensive analysis\n",
    "    feature_analysis, file_analysis, global_stats = analyze_dataset_comprehensively(\n",
    "        available_files, \n",
    "        max_files=None, \n",
    "        samples_per_file=None\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ ANALYSIS RESULTS SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"âœ… Total samples analyzed: {global_stats['total_samples_analyzed']}\")\n",
    "    print(f\"ğŸ”§ Unique features discovered: {len(global_stats['unique_features'])}\")\n",
    "    print(f\"âš ï¸  Total parsing errors: {global_stats['total_parsing_errors']}\")\n",
    "    \n",
    "    # Feature consistency analysis\n",
    "    print(f\"\\nğŸ” FEATURE CONSISTENCY ANALYSIS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for feature_name, info in feature_analysis.items():\n",
    "        unique_shapes = set(str(shape) for shape in info['shapes'])\n",
    "        unique_types = set(info['types'])\n",
    "        \n",
    "        consistency_status = \"âœ…\" if len(unique_shapes) == 1 and len(unique_types) == 1 else \"âš ï¸ \"\n",
    "        \n",
    "        print(f\"{consistency_status} {feature_name}:\")\n",
    "        print(f\"    Shapes: {list(unique_shapes)}\")\n",
    "        print(f\"    Types: {list(unique_types)}\")\n",
    "        print(f\"    Found in: {len(set(info['files_found_in']))} file(s)\")\n",
    "        \n",
    "        # Value range information if available\n",
    "        if info['statistics']['min_vals'] and info['statistics']['max_vals']:\n",
    "            min_val = min(info['statistics']['min_vals'])\n",
    "            max_val = max(info['statistics']['max_vals'])\n",
    "            print(f\"    Value range: [{min_val:.3f}, {max_val:.3f}]\")\n",
    "        \n",
    "        # Data quality issues\n",
    "        total_nans = sum(info['statistics'].get('nan_counts', [0]))\n",
    "        total_infs = sum(info['statistics'].get('inf_counts', [0]))\n",
    "        \n",
    "        if total_nans > 0 or total_infs > 0:\n",
    "            print(f\"    âš ï¸  Quality issues: {total_nans} NaNs, {total_infs} Infs\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Save comprehensive analysis results\n",
    "    analysis_results_file = DATA_DIR / \"interim\" / \"comprehensive_feature_analysis.pkl\"\n",
    "    \n",
    "    with open(analysis_results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'feature_analysis': feature_analysis,\n",
    "            'file_analysis': file_analysis,\n",
    "            'global_stats': global_stats,\n",
    "            'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'analysis_parameters': {\n",
    "                'max_files_analyzed': len(available_files),\n",
    "                'samples_per_file': 'all',\n",
    "                'total_files_available': len(available_files)\n",
    "            }\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Comprehensive analysis saved to: {analysis_results_file}\")\n",
    "    \n",
    "    # Generate summary report\n",
    "    summary_report_path = DATA_DIR / \"processed\" / \"dataset_exploration_report.txt\"\n",
    "    summary_report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(summary_report_path, 'w') as f:\n",
    "        f.write(\"WILDFIRE DATASET EXPLORATION REPORT\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        f.write(f\"Analysis Date: {pd.Timestamp.now()}\\n\")\n",
    "        f.write(f\"Dataset Location: {TFRECORD_DATA_DIR}\\n\")\n",
    "        f.write(f\"Total Files Available: {len(available_files)}\\n\")\n",
    "        f.write(f\"Files Analyzed: {len(available_files)}\\n\")\n",
    "        f.write(f\"Total Samples Analyzed: {global_stats['total_samples_analyzed']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"FEATURE SUMMARY:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for feature_name, info in feature_analysis.items():\n",
    "            f.write(f\"{feature_name}:\\n\")\n",
    "            f.write(f\"  - Shapes: {set(str(s) for s in info['shapes'])}\\n\")\n",
    "            f.write(f\"  - Types: {set(info['types'])}\\n\")\n",
    "            f.write(f\"  - Found in {len(set(info['files_found_in']))} files\\n\")\n",
    "            \n",
    "            # Statistical summary\n",
    "            if info['statistics']['min_vals']:\n",
    "                min_val = min(info['statistics']['min_vals'])\n",
    "                max_val = max(info['statistics']['max_vals'])\n",
    "                mean_val = np.mean(info['statistics']['mean_vals'])\n",
    "                f.write(f\"  - Range: [{min_val:.4f}, {max_val:.3f}], Mean: {mean_val:.3f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"ğŸ“„ Summary report saved to: {summary_report_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No TFRecord files found for analysis\")\n",
    "\n",
    "\n",
    "# Analyze multiple examples to understand data structure consistency\n",
    "def analyze_tfrecord_structure(file_path, num_examples=5):\n",
    "    \"\"\"Analyze structure across multiple examples.\"\"\"\n",
    "    dataset = tf.data.TFRecordDataset(str(file_path))\n",
    "    \n",
    "    feature_info = {}\n",
    "    \n",
    "    for i, example in enumerate(dataset.take(num_examples)):\n",
    "        parsed = parse_tfrecord_example_robust(example)\n",
    "        \n",
    "        # Skip if there was a global parsing error\n",
    "        if '_global_error' in parsed:\n",
    "            continue\n",
    "            \n",
    "        for feature_name, info in parsed.items():\n",
    "            # Skip internal parsing metadata\n",
    "            if feature_name.startswith('_'):\n",
    "                continue\n",
    "                \n",
    "            if feature_name not in feature_info:\n",
    "                feature_info[feature_name] = {\n",
    "                    'type': info.get('type', 'unknown'),\n",
    "                    'shapes': [],\n",
    "                    'min_vals': [],\n",
    "                    'max_vals': [],\n",
    "                    'mean_vals': []\n",
    "                }\n",
    "            \n",
    "            feature_info[feature_name]['shapes'].append(info.get('shape', 'unknown'))\n",
    "            \n",
    "            # Calculate statistics for tensor data\n",
    "            if info.get('type') == 'tensor_float32' and info.get('data') is not None:\n",
    "                data = info['data']\n",
    "                if hasattr(data, 'size') and data.size > 0:\n",
    "                    feature_info[feature_name]['min_vals'].append(float(np.min(data)))\n",
    "                    feature_info[feature_name]['max_vals'].append(float(np.max(data)))\n",
    "                    feature_info[feature_name]['mean_vals'].append(float(np.mean(data)))\n",
    "    \n",
    "    return feature_info\n",
    "\n",
    "# Analyze structure if files are available\n",
    "if tfrecord_files:\n",
    "    print(\"Analyzing data structure across multiple examples...\")\n",
    "    structure_info = analyze_tfrecord_structure(tfrecord_files[0], num_examples=3)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for feature_name, info in structure_info.items():\n",
    "        shapes_consistent = len(set(map(str, info['shapes']))) == 1\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Feature': feature_name,\n",
    "            'Type': info['type'],\n",
    "            'Shape': info['shapes'][0] if shapes_consistent else f\"Variable: {info['shapes']}\",\n",
    "            'Shape_Consistent': shapes_consistent,\n",
    "            'Min_Value': np.min(info['min_vals']) if info['min_vals'] else 'N/A',\n",
    "            'Max_Value': np.max(info['max_vals']) if info['max_vals'] else 'N/A',\n",
    "            'Mean_Value': np.mean(info['mean_vals']) if info['mean_vals'] else 'N/A'\n",
    "        })\n",
    "    \n",
    "    structure_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\nğŸ“Š TFRecord Structure Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(structure_df.to_string(index=False, max_colwidth=20))\n",
    "    \n",
    "    # Check for inconsistencies\n",
    "    inconsistent_features = structure_df[~structure_df['Shape_Consistent']]\n",
    "    if not inconsistent_features.empty:\n",
    "        print(f\"\\nâš ï¸  Features with inconsistent shapes:\")\n",
    "        for _, row in inconsistent_features.iterrows():\n",
    "            print(f\"  - {row['Feature']}: {row['Shape']}\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… All features have consistent shapes across examples\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No TFRecord files found for structure analysis\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Key features identified:\")\n",
    "for feature in ['elevation', 'tmmn', 'tmmx', 'pr', 'fm100', 'fm1000', 'erc', 'PrevFireMask', 'FireMask']:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# Feature names discovered from the first pass\n",
    "discovered_features = [\n",
    "    'wdir_wind', 'fuel1', 'bi', 'gust_med', 'avg_sph', 'wdir_gust', 'wind_75',\n",
    "    'burn_index_tc', 'pr', 'vs', 'psi', 'burning_index_tc', 'population',\n",
    "    'erc', 'rmax', 'rmin', 'NDVI', 'PrevFireMask', 'elevation', 'th',\n",
    "    'vpd', 'sph', 'tmmn', 'tmmx', 'FireMask', 'impervious', 'water',\n",
    "    'viirs_FireMask', 'viirs_PrevFireMask'\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ¯ COMPREHENSIVE FEATURE ANALYSIS & RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Execute analysis if comprehensive results are available\n",
    "if 'feature_analysis' in locals() and feature_analysis:\n",
    "    # Get comprehensive feature descriptions\n",
    "    feature_descriptions = create_comprehensive_feature_descriptions()\n",
    "    \n",
    "    # Generate preprocessing recommendations\n",
    "    preprocessing_recommendations = generate_preprocessing_recommendations(\n",
    "        feature_analysis, feature_descriptions\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“Š FEATURE CATEGORIZATION\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Categorize features by type and importance\n",
    "    categories = {}\n",
    "    for feature_name in feature_analysis.keys():\n",
    "        desc = feature_descriptions.get(feature_name, {})\n",
    "        category = desc.get('category', 'unknown')\n",
    "        importance = desc.get('importance', 'medium')\n",
    "        \n",
    "        if category not in categories:\n",
    "            categories[category] = {}\n",
    "        if importance not in categories[category]:\n",
    "            categories[category][importance] = []\n",
    "        \n",
    "        categories[category][importance].append(feature_name)\n",
    "    \n",
    "    # Display categorization\n",
    "    for category, importance_dict in categories.items():\n",
    "        print(f\"\\nğŸ·ï¸  {category.upper()}:\")\n",
    "        for importance, features in importance_dict.items():\n",
    "            print(f\"  {importance.title()}: {len(features)} features\")\n",
    "            if len(features) <= 4:\n",
    "                print(f\"    â€¢ \" + \"\\n    â€¢ \".join(features))\n",
    "            else:\n",
    "                print(f\"    â€¢ \" + \"\\n    â€¢ \".join(features[:3]) + f\"\\n    â€¢ ... and {len(features)-3} more\")\n",
    "    \n",
    "    print(f\"\\nğŸ”§ PREPROCESSING RECOMMENDATIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Display normalization strategies\n",
    "    norm_strategies = {}\n",
    "    for feature, strategy in preprocessing_recommendations['normalization_strategies'].items():\n",
    "        if strategy not in norm_strategies:\n",
    "            norm_strategies[strategy] = []\n",
    "        norm_strategies[strategy].append(feature)\n",
    "    \n",
    "    print(\"ğŸ“ˆ Normalization Strategies:\")\n",
    "    for strategy, features in norm_strategies.items():\n",
    "        print(f\"  â€¢ {strategy.replace('_', ' ').title()}: {len(features)} features\")\n",
    "        if len(features) <= 3:\n",
    "            print(f\"    - {', '.join(features)}\")\n",
    "    \n",
    "    # Display feature engineering opportunities\n",
    "    if preprocessing_recommendations['feature_engineering_opportunities']:\n",
    "        print(f\"\\nğŸ’¡ Feature Engineering Opportunities:\")\n",
    "        for opp in preprocessing_recommendations['feature_engineering_opportunities']:\n",
    "            print(f\"  â€¢ {opp['description']}\")\n",
    "            print(f\"    Features: {', '.join(opp['features_involved'])}\")\n",
    "    \n",
    "    # Display data quality issues\n",
    "    if preprocessing_recommendations['data_quality_issues']:\n",
    "        print(f\"\\nâš ï¸  Data Quality Issues Found:\")\n",
    "        for issue in preprocessing_recommendations['data_quality_issues']:\n",
    "            severity_icon = \"ğŸš¨\" if issue['severity'] == 'high' else \"âš ï¸ \"\n",
    "            print(f\"  {severity_icon} {issue['feature']}: {issue['issue']}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Comprehensive feature analysis and recommendations complete!\")\n",
    "    print(f\"ğŸ“Š Analysis covers {len(feature_analysis)} features across {global_stats['total_samples_analyzed']} samples\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Comprehensive analysis not available - using discovered feature list\")\n",
    "    print(f\"ğŸ” Found {len(discovered_features)} features from initial exploration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
